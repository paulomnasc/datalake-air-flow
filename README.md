# üöÄ Solu√ß√£o H√≠brida: Apache Airflow + PostgreSQL + MinIO

Este projeto integra tr√™s componentes principais para orquestra√ß√£o de dados e armazenamento:

- **Apache Airflow**: Orquestra√ß√£o de workflows
- **PostgreSQL**: Banco de dados relacional para metadados do Airflow
- **MinIO**: Armazenamento de objetos compat√≠vel com S3

A base foi clonada do reposit√≥rio do Adriano e adaptada para incluir os tr√™s servi√ßos integrados. Os artefatos de c√≥digo (DAGs, scripts, configura√ß√µes) est√£o versionados neste reposit√≥rio.

---

## Configura√ß√£o m√≠nima de hardware

Os requisitos m√≠nimos de hardware para um Apache Airflow b√°sico s√£o de 10GB de HD, 4GB de RAM e 2 CPUs. 

√â importante notar que, para uma implanta√ß√£o em produ√ß√£o, pode ser necess√°rio mais hardware, dependendo da carga de trabalho, e que o Airflow tamb√©m pode rodar em ambientes como Kubernetes ou nuvem, com requisitos que variam de acordo com a plataforma escolhida. 
Requisitos m√≠nimos (para ambientes de teste/desenvolvimento):

Disco: 10 GB de HD.
Mem√≥ria: 4 GB de RAM.
Processador (CPU): 2 CPUs (ou VCPUs). 
Requisitos recomendados e considera√ß√µes adicionais:
Banco de dados: O Airflow precisa de um banco de dados de metadados para funcionar. 
√â recomendado um banco de dados como PostgreSQL ou MySQL. 

Ambiente virtual: Para evitar conflitos de depend√™ncias, √© altamente recomend√°vel usar um ambiente virtual (como o venv ou conda). 
Sistema operacional: Embora o Airflow possa ser instalado em Windows, ele funciona melhor em um ambiente tipo Unix, como o Linux, que pode ser executado nativamente ou atrav√©s do Subsistema do Windows para Linux (WSL). 

Ambientes de nuvem: Se for usar servi√ßos como Amazon MWAA, os requisitos de hardware s√£o vari√°veis e o pagamento √© por uso. Os custos e recursos dependem do n√≠vel de uso. 

## üìÅ Estrutura do Projeto

```
airflow-spark-minio-postgres/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ entrypoint.sh
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ dags/
        ‚îî‚îÄ‚îÄ suas_dags.py
```

---

## ‚öôÔ∏è Etapas de Implanta√ß√£o

### 1. Clonar o Projeto

```bash
git clone https://github.com/paulomnasc/datalake-air-flow.git
cd datalake-air-flow
```

> Substitua o link acima pelo reposit√≥rio real, se necess√°rio.

---

### 2. Build e Inicializa√ß√£o dos Containers

```bash
chmod +x entrypoint.sh
docker-compose down --remove-orphans
docker-compose build
docker-compose up -d
```

> Neste momento:
> - O **PostgreSQL** √© instanciado com o banco `airflow`, usu√°rio `airflow` e senha `airflow`
> - O **MinIO** √© iniciado com o volume `/data` e console web na porta 9001
> - O **Airflow Webserver e Scheduler** s√£o constru√≠dos e iniciados com base nas vari√°veis de ambiente

---

### 2.1 Passo opcional de verifica√ß√£o se o Airflow est√° Up (opercional)
```bash
docker exec -it airflow-webserver airflow dags list

```

### 3. Inicializar o Banco de Dados do Airflow (! Apenas novas instala√ß√µes)

```bash
docker exec -it airflow-webserver airflow db init
```

> Esse comando aplica as migra√ß√µes e cria as tabelas no banco `airflow` do PostgreSQL.

---

### 4. Criar Usu√°rio Admin no Airflow (! Apenas novas instala√ß√µes)

Via CLI:

```bash
docker exec -it airflow-webserver airflow users create \
  --username admin \
  --firstname Air \
  --lastname Flow \
  --role Admin \
  --email admin@example.com \
  --password admin
```

Ou via DAG j√° inclu√≠da no projeto.

---

## üåê Consoles Administrativas e Acesso

| Servi√ßo             | Endere√ßo de Acesso                     | Porta | Usu√°rio / Senha           | Banco de Dados     | Observa√ß√µes                          |
|---------------------|----------------------------------------|-------|----------------------------|--------------------|--------------------------------------|
| **Airflow UI**      | [http://localhost:8085](http://localhost:8085) | 8085  | `admin` / `admin`          | ‚Äî                  | Criado ap√≥s `airflow db init` e `users create` |
| **MinIO Console**   | [http://localhost:9001](http://localhost:9001) | 9001  | `admin` / `admin123`| ‚Äî                  | Interface web de armazenamento S3   |
| **MinIO API S3**    | `http://localhost:9000`                | 9000  | `admin` / `admin123`| ‚Äî                  | Usado por boto3, S3Hook, etc.        |
| **PostgreSQL**      | via cliente externo ou terminal        | 5432  | `airflow` / `airflow`      | `airflow`          | Banco de metadados do Airflow        |

---

## üß™ Testes de Acesso

### Airflow:

```bash
curl http://localhost:8085
```

### MinIO:

```bash
curl http://localhost:9001
```

### PostgreSQL via terminal:

```bash
docker exec -it postgres psql -U airflow -d airflow
```

---

## ‚úÖ Status Final

Com essa implanta√ß√£o:

- Airflow est√° orquestrando suas DAGs com interface acess√≠vel
- MinIO est√° dispon√≠vel como armazenamento S3 local
- PostgreSQL est√° persistindo os metadados e acess√≠vel via terminal ou cliente gr√°fico
- Todos os servi√ßos est√£o integrados e prontos para produ√ß√£o ou desenvolvimento local

### Configurando o Airflow para conectar no MinIO

## üîó Conex√£o Airflow com MinIO (`minio_conn`)

Para que o Airflow consiga enviar arquivos para o MinIO usando `S3Hook`, √© necess√°rio configurar uma conex√£o do tipo **Amazon S3** com os seguintes par√¢metros:

### üìã Detalhes da conex√£o

- **Conn Id**: `minio_conn`
- **Conn Type**: `Amazon Web Serices`
- **Login**: `admin` *(Access Key do MinIO)*
- **Password**: `admin123` *(Secret Key do MinIO)*

### ‚öôÔ∏è Campo Extra (JSON)

```json
{
  "host": "http://minio:9000",
  "port": 9000,
  "secure": false
}
```

### Utilidades

### ‚úÖ 6. Reiniciar o Airflow

Se for necess√°rio, reinicie o Airflow:

```bash
docker restart airflow-webserver
```
**O comando mais direto para verificar se o Airflow carregou totalmente √©:**

```bash
docker logs <nome_do_container_airflow>
```

Por exemplo, se estiver usando Docker Compose e seu servi√ßo se chama `airflow`, voc√™ pode usar:

```bash
docker logs datalake-local_airflow_1
```

---

### üß© O que procurar nos logs

Voc√™ saber√° que o Airflow carregou com sucesso quando encontrar mensagens como:

```
Scheduler started...
Starting webserver at http://0.0.0.0:8080
```

Essas mensagens indicam que tanto o *scheduler* quanto o *webserver* est√£o ativos e prontos.

---

### ‚úÖ Alternativas √∫teis

Se estiver usando o Airflow fora de containers, voc√™ pode verificar com:

```bash
airflow webserver
```

ou

```bash
airflow scheduler
```

E observar no terminal se os servi√ßos iniciam sem erros.
---

Navegar um recurso com interface amig√°vel 

```bash
mc ls local/lab01/processed/raw/
```
